{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“š Notebook 01: Introduction and Fundamentals\n",
    "\n",
    "**LangChain 1.0.5+ | Mixed Level Class**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "1. What LangChain is and why it's useful\n",
    "2. LangChain's modular architecture\n",
    "3. Core concepts: Documents, Chains, and LCEL\n",
    "4. How to set up your development environment\n",
    "5. The difference between LangChain and traditional ML pipelines\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“– Table of Contents\n",
    "\n",
    "1. [What is LangChain?](#what-is-langchain)\n",
    "2. [LangChain Architecture](#architecture)\n",
    "3. [Environment Setup](#setup)\n",
    "4. [Core Concepts](#core-concepts)\n",
    "5. [Quick Start Example](#quick-start)\n",
    "6. [LangChain vs Traditional Pipelines](#comparison)\n",
    "7. [Summary & Next Steps](#summary)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"what-is-langchain\"></a>\n",
    "## 1. What is LangChain? ğŸ¤”\n",
    "\n",
    "### ğŸ”° BEGINNER SECTION\n",
    "\n",
    "**LangChain** is an open-source framework that makes it easy to build applications powered by Large Language Models (LLMs) like GPT-4, Claude, or Llama.\n",
    "\n",
    "### Why LangChain?\n",
    "\n",
    "Imagine you want to build a chatbot that can:\n",
    "- Answer questions about your company's PDF documents\n",
    "- Search through your database\n",
    "- Remember previous conversations\n",
    "- Call external APIs\n",
    "\n",
    "Without LangChain, you'd need to:\n",
    "1. âœï¸ Write code to load and parse PDFs\n",
    "2. âœï¸ Convert text to embeddings\n",
    "3. âœï¸ Store embeddings in a vector database\n",
    "4. âœï¸ Implement semantic search\n",
    "5. âœï¸ Format prompts for the LLM\n",
    "6. âœï¸ Handle LLM API calls\n",
    "7. âœï¸ Manage conversation memory\n",
    "\n",
    "**With LangChain:**\n",
    "- âœ… All these components are pre-built and ready to use\n",
    "- âœ… You just connect them like LEGO blocks\n",
    "- âœ… Focus on your application logic, not infrastructure\n",
    "\n",
    "### ğŸ“ INTERMEDIATE NOTE\n",
    "\n",
    "LangChain provides:\n",
    "- **Abstractions**: Unified interfaces for different LLMs, vector stores, and tools\n",
    "- **Chains**: Composable workflows using LCEL (LangChain Expression Language)\n",
    "- **Agents**: Autonomous systems that can use tools and make decisions\n",
    "- **Memory**: Conversation history and context management\n",
    "- **Callbacks**: Monitoring, logging, and debugging hooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"architecture\"></a>\n",
    "## 2. LangChain Architecture ğŸ—ï¸\n",
    "\n",
    "### ğŸ”° BEGINNER: Visual Architecture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    YOUR APPLICATION                     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                           â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                  LANGCHAIN FRAMEWORK                    â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚  â”‚ Document â”‚  â”‚   Text   â”‚  â”‚ Embeddingsâ”‚  â”‚ Vector  â”‚ â”‚\n",
    "â”‚  â”‚ Loaders  â”‚â†’ â”‚ Splittersâ”‚â†’ â”‚  Models   â”‚â†’ â”‚ Stores  â”‚ â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚\n",
    "â”‚  â”‚Retrieversâ”‚  â”‚ Prompts  â”‚  â”‚   LLMs   â”‚               â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
    "â”‚  â”‚         LCEL (LangChain Expression Language)     â”‚   â”‚\n",
    "â”‚  â”‚         Connects everything with | operator      â”‚   â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                           â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚             EXTERNAL SERVICES & DATA                    â”‚\n",
    "â”‚  â€¢ OpenAI/Anthropic APIs  â€¢ Vector Databases            â”‚\n",
    "â”‚  â€¢ PDF Files              â€¢ Websites                    â”‚\n",
    "â”‚  â€¢ Databases              â€¢ APIs                        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### ğŸ“ INTERMEDIATE: Package Structure (LangChain 1.0+)\n",
    "\n",
    "LangChain is organized into several packages:\n",
    "\n",
    "| Package | Purpose | Example Imports |\n",
    "|---------|---------|------------------|\n",
    "| **langchain-core** | Core abstractions, base classes | `from langchain_core.documents import Document` |\n",
    "| **langchain-community** | Community integrations (loaders, vector stores) | `from langchain_community.document_loaders import PyPDFLoader` |\n",
    "| **langchain-openai** | OpenAI-specific integrations | `from langchain_openai import ChatOpenAI, OpenAIEmbeddings` |\n",
    "| **langchain-text-splitters** | Text splitting utilities | `from langchain_text_splitters import RecursiveCharacterTextSplitter` |\n",
    "\n",
    "**Why this matters:** In LangChain 1.0+, you import from specific packages instead of `langchain` directly. This reduces dependencies and improves modularity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "## 3. Environment Setup ğŸ› ï¸\n",
    "\n",
    "### ğŸ”° BEGINNER: Step-by-Step Setup\n",
    "\n",
    "Let's verify your environment is ready!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.6 | packaged by conda-forge | (main, Oct  3 2023, 10:37:07) [Clang 15.0.7 ]\n",
      "âœ… Python version is compatible\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Check Python version (should be 3.9+)\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Check if Python is 3.9 or higher\n",
    "if sys.version_info >= (3, 9):\n",
    "    print(\"âœ… Python version is compatible\")\n",
    "else:\n",
    "    print(\"âŒ Please upgrade to Python 3.9 or higher\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain version: 1.0.5\n",
      "LangChain Core version: 1.0.5\n",
      "âœ… LangChain 1.0+ detected\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Import LangChain and check version\n",
    "import langchain\n",
    "from langchain_core import __version__ as core_version\n",
    "\n",
    "print(f\"LangChain version: {langchain.__version__}\")\n",
    "print(f\"LangChain Core version: {core_version}\")\n",
    "\n",
    "# We're using LangChain 1.0.5+ for this course\n",
    "if langchain.__version__ >= \"1.0\":\n",
    "    print(\"âœ… LangChain 1.0+ detected\")\n",
    "else:\n",
    "    print(\"âŒ Please upgrade: pip install --upgrade langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OPENAI_API_KEY found\n",
      "âœ… GOOGLE_API_KEY found\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Load environment variables (API keys)\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Check if OpenAI API key is set\n",
    "# NOTE: We don't print the actual key for security!\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"âœ… OPENAI_API_KEY found\")\n",
    "else:\n",
    "    print(\"âŒ OPENAI_API_KEY not found\")\n",
    "    print(\"   Create a .env file with: OPENAI_API_KEY=your-key-here\")\n",
    "\n",
    "# Check for Google API key (optional, for Google Gemini)\n",
    "if os.getenv(\"GOOGLE_API_KEY\"):\n",
    "    print(\"âœ… GOOGLE_API_KEY found\")\n",
    "else:\n",
    "    print(\"âš ï¸  GOOGLE_API_KEY not found (optional for this notebook)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ Setting Up Your .env File\n",
    "\n",
    "If you don't have a `.env` file, create one in the project root:\n",
    "\n",
    "```bash\n",
    "# .env file\n",
    "OPENAI_API_KEY=sk-proj-your-key-here\n",
    "GOOGLE_API_KEY=your-google-key-here\n",
    "```\n",
    "\n",
    "**âš ï¸ SECURITY WARNING:**\n",
    "- Never commit `.env` files to Git\n",
    "- Add `.env` to your `.gitignore` file\n",
    "- Never hardcode API keys in your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"core-concepts\"></a>\n",
    "## 4. Core Concepts ğŸ“š\n",
    "\n",
    "### 4.1 Documents ğŸ“„\n",
    "\n",
    "### ğŸ”° BEGINNER\n",
    "\n",
    "A **Document** is LangChain's way of representing a piece of text with metadata.\n",
    "\n",
    "Think of it like a note card:\n",
    "- **Front (page_content):** The actual text\n",
    "- **Back (metadata):** Information about the text (source, page number, date, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content:\n",
      "LangChain makes building LLM applications easy!\n",
      "\n",
      "Metadata:\n",
      "{'source': 'introduction.txt', 'author': 'LangChain Team', 'date': '2025-01-16'}\n",
      "\n",
      "Source: introduction.txt\n"
     ]
    }
   ],
   "source": [
    "# Creating a Document manually\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Create a simple document\n",
    "doc = Document(\n",
    "    page_content=\"LangChain makes building LLM applications easy!\",\n",
    "    metadata={\n",
    "        \"source\": \"introduction.txt\",\n",
    "        \"author\": \"LangChain Team\",\n",
    "        \"date\": \"2025-01-16\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Access the content\n",
    "print(\"Content:\")\n",
    "print(doc.page_content)\n",
    "print(\"\\nMetadata:\")\n",
    "print(doc.metadata)\n",
    "print(f\"\\nSource: {doc.metadata['source']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ INTERMEDIATE: Why Metadata Matters\n",
    "\n",
    "Metadata is crucial for:\n",
    "1. **Citation**: Showing users where information came from\n",
    "2. **Filtering**: Only search documents from specific sources\n",
    "3. **Debugging**: Tracking which documents are being retrieved\n",
    "4. **Analytics**: Understanding which sources are most useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“„ Document 1:\n",
      "   Content: Python is a high-level programming language.\n",
      "   Category: programming\n",
      "   Difficulty: beginner\n",
      "\n",
      "ğŸ“„ Document 2:\n",
      "   Content: Machine learning is a subset of artificial intelligence (AI).\n",
      "   Category: AI\n",
      "   Difficulty: intermediate\n",
      "\n",
      "ğŸ“„ Document 3:\n",
      "   Content: RAG combines retrieval and generation for better LLM outputs.\n",
      "   Category: AI\n",
      "   Difficulty: advanced\n"
     ]
    }
   ],
   "source": [
    "# Creating multiple documents (like a mini knowledge base)\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"Python is a high-level programming language.\",\n",
    "        metadata={\"category\": \"programming\", \"difficulty\": \"beginner\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Machine learning is a subset of artificial intelligence (AI).\",\n",
    "        metadata={\"category\": \"AI\", \"difficulty\": \"intermediate\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"RAG combines retrieval and generation for better LLM outputs.\",\n",
    "        metadata={\"category\": \"AI\", \"difficulty\": \"advanced\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "# Print all documents\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    print(f\"\\nğŸ“„ Document {i}:\")\n",
    "    print(f\"   Content: {doc.page_content}\")\n",
    "    print(f\"   Category: {doc.metadata['category']}\")\n",
    "    print(f\"   Difficulty: {doc.metadata['difficulty']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 LCEL (LangChain Expression Language) ğŸ”—\n",
    "\n",
    "### ğŸ”° BEGINNER\n",
    "\n",
    "**LCEL** is a way to connect different components using the pipe operator `|`.\n",
    "\n",
    "Think of it like a factory assembly line:\n",
    "```\n",
    "Input â†’ Component 1 â†’ Component 2 â†’ Component 3 â†’ Output\n",
    "```\n",
    "\n",
    "### Before LCEL (Old Way - Messy!):\n",
    "```python\n",
    "output = component3(component2(component1(input)))\n",
    "```\n",
    "\n",
    "### With LCEL (New Way - Clean!):\n",
    "```python\n",
    "chain = component1 | component2 | component3\n",
    "output = chain.invoke(input)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ INTERMEDIATE: LCEL Deep Dive\n",
    "\n",
    "LCEL provides:\n",
    "- **Streaming**: Stream outputs as they're generated\n",
    "- **Batch Processing**: Process multiple inputs efficiently\n",
    "- **Async Support**: Non-blocking operations\n",
    "- **Debugging**: Better error messages and logging\n",
    "- **Type Safety**: Better IDE autocomplete\n",
    "\n",
    "**Key Methods:**\n",
    "- `.invoke(input)`: Process single input\n",
    "- `.batch([input1, input2])`: Process multiple inputs\n",
    "- `.stream(input)`: Stream output tokens\n",
    "- `.ainvoke(input)`: Async version of invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 'hello langchain'\n",
      "\n",
      "Processing:\n",
      "  Step 1: uppercase â†’ HELLO LANGCHAIN\n",
      "  Step 2: add_prefix â†’ RESULT AFTER ADDING PREFIX: HELLO LANGCHAIN\n",
      "  Step 3: add_emoji â†’ âœ… RESULT AFTER ADDING PREFIX: HELLO LANGCHAIN\n",
      "\n",
      "Final Output: âœ… RESULT AFTER ADDING PREFIX: HELLO LANGCHAIN\n"
     ]
    }
   ],
   "source": [
    "# Simple LCEL Example: String Transformation Chain\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Create simple transformation functions\n",
    "def uppercase(text: str) -> str:\n",
    "    \"\"\"Convert text to uppercase\"\"\"\n",
    "    print(f\"  Step 1: uppercase â†’ {text.upper()}\")\n",
    "    return text.upper()\n",
    "\n",
    "def add_prefix(text: str) -> str:\n",
    "    \"\"\"Add a prefix to text\"\"\"\n",
    "    result = f\"RESULT AFTER ADDING PREFIX: {text}\"\n",
    "    print(f\"  Step 2: add_prefix â†’ {result}\")\n",
    "    return result\n",
    "\n",
    "def add_emoji(text: str) -> str:\n",
    "    \"\"\"Add emoji to text\"\"\"\n",
    "    result = f\"âœ… {text}\"\n",
    "    print(f\"  Step 3: add_emoji â†’ {result}\")\n",
    "    return result\n",
    "\n",
    "# Create runnables (components that can be chained)\n",
    "uppercase_runnable = RunnableLambda(uppercase)\n",
    "prefix_runnable = RunnableLambda(add_prefix)\n",
    "emoji_runnable = RunnableLambda(add_emoji)\n",
    "\n",
    "# Build the chain using LCEL\n",
    "chain = uppercase_runnable | prefix_runnable | emoji_runnable\n",
    "\n",
    "# Execute the chain\n",
    "print(\"Input: 'hello langchain'\")\n",
    "print(\"\\nProcessing:\")\n",
    "result = chain.invoke(\"hello langchain\")\n",
    "print(f\"\\nFinal Output: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Chains ğŸ”—\n",
    "\n",
    "### ğŸ”° BEGINNER\n",
    "\n",
    "A **Chain** is a sequence of operations. Common chain types:\n",
    "\n",
    "1. **Simple Chain**: Input â†’ Process â†’ Output\n",
    "2. **RAG Chain**: Query â†’ Retrieve â†’ Generate â†’ Answer\n",
    "3. **Multi-step Chain**: Input â†’ Step 1 â†’ Step 2 â†’ Step 3 â†’ Output\n",
    "\n",
    "We'll build complex chains in later notebooks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"quick-start\"></a>\n",
    "## 5. Quick Start Example ğŸš€\n",
    "\n",
    "### ğŸ”° BEGINNER: Your First LLM Call\n",
    "\n",
    "Let's make our first call to an LLM using LangChain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is LangChain in one sentence?\n",
      "\n",
      "Answer: LangChain is a decentralized language learning platform that uses blockchain technology to connect language learners with native speakers for personalized lessons.\n",
      "Elapsed time: 0.8909 seconds\n"
     ]
    }
   ],
   "source": [
    "# Import ChatOpenAI (the LLM interface)\n",
    "from langchain_openai import ChatOpenAI\n",
    "import time\n",
    "\n",
    "start_time = time.time()  \n",
    "# Initialize the LLM\n",
    "# model: Which GPT model to use\n",
    "# temperature: 0 = deterministic, 1 = creative\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",  # Cheaper, faster model for learning\n",
    "    temperature=0  # Deterministic outputs for learning\n",
    ")\n",
    "\n",
    "# Make a simple call\n",
    "response = llm.invoke(\"What is LangChain in one sentence?\")\n",
    "\n",
    "# Print the response\n",
    "print(\"Question: What is LangChain in one sentence?\")\n",
    "print(f\"\\nAnswer: {response.content}\")\n",
    "\n",
    "end_time = time.time()     # End time\n",
    "\n",
    "elapsed = end_time - start_time\n",
    "print(f\"Elapsed time: {elapsed:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is LangChain in one sentence?\n",
      "\n",
      "Answer: LangChain is a decentralized platform that connects language learners with native speakers for language exchange and tutoring.\n",
      "Elapsed time: 0.7993 seconds\n"
     ]
    }
   ],
   "source": [
    "# Initialize the LLM\n",
    "# model: Which GPT model to use\n",
    "# temperature: 0 = deterministic, 1 = creative\n",
    "st = time.time()\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",  # Cheaper, faster model for learning\n",
    "    temperature=0.4,  # Slightly creative temperature\n",
    ")\n",
    "\n",
    "# Make a simple call\n",
    "response = llm.invoke(\"What is LangChain in one sentence?\")\n",
    "\n",
    "# Print the response\n",
    "print(\"Question: What is LangChain in one sentence?\")\n",
    "print(f\"\\nAnswer: {response.content}\")\n",
    "\n",
    "et = time.time()\n",
    "telapsed = et - st\n",
    "print(f\"Elapsed time: {telapsed:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is LangChain in one sentence?\n",
      "\n",
      "Answer: LangChain is a development framework that helps build applications powered by large language models (LLMs) by enabling them to connect with external data sources and tools, and chain together complex operations.\n",
      "Elapsed time: 9.3289 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import ChatOpenAI (the LLM interface)\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "stg = time.time()\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",\n",
    "                             temperature=0)\n",
    "# Initialize the LLM\n",
    "# model: Which GPT model to use\n",
    "# temperature: 0 = deterministic, 1 = creative\n",
    "\n",
    "# Make a simple call\n",
    "response_g = llm.invoke(\"What is LangChain in one sentence?\")\n",
    "\n",
    "# Print the response\n",
    "print(\"Question: What is LangChain in one sentence?\")\n",
    "print(f\"\\nAnswer: {response.content}\")\n",
    "\n",
    "etg = time.time()\n",
    "elapsed_g = etg - stg\n",
    "print(f\"Elapsed time: {elapsed_g:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ INTERMEDIATE: Understanding the Response Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response object for OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Type: <class 'langchain_core.messages.ai.AIMessage'>\n",
      "\n",
      "Content: LangChain is a decentralized platform that connects language learners with native speakers for language exchange and tutoring.\n",
      "\n",
      "Response Metadata:\n",
      "{'token_usage': {'completion_tokens': 19, 'prompt_tokens': 15, 'total_tokens': 34, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CcyTLHgdqtyZJXYdFwiIBdq2WgFuC', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}\n",
      "\n",
      "Tokens Used:\n",
      "  Prompt: 15\n",
      "  Completion: 19\n",
      "  Total: 34\n"
     ]
    }
   ],
   "source": [
    "# The response is an AIMessage object with metadata\n",
    "print(\"Response Type:\", type(response))\n",
    "print(\"\\nContent:\", response.content)\n",
    "print(\"\\nResponse Metadata:\")\n",
    "print(response.response_metadata)\n",
    "\n",
    "# Access specific metadata\n",
    "if 'token_usage' in response.response_metadata:\n",
    "    usage = response.response_metadata['token_usage']\n",
    "    print(f\"\\nTokens Used:\")\n",
    "    print(f\"  Prompt: {usage.get('prompt_tokens', 'N/A')}\")\n",
    "    print(f\"  Completion: {usage.get('completion_tokens', 'N/A')}\")\n",
    "    print(f\"  Total: {usage.get('total_tokens', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response object for Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Type: <class 'langchain_core.messages.ai.AIMessage'>\n",
      "\n",
      "Content: LangChain is a development framework that helps build applications powered by large language models (LLMs) by enabling them to connect with external data sources and tools, and chain together complex operations.\n",
      "\n",
      "Response Metadata:\n",
      "{'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}\n"
     ]
    }
   ],
   "source": [
    "# The response is an AIMessage object with metadata\n",
    "print(\"Response Type:\", type(response_g))\n",
    "print(\"\\nContent:\", response_g.content)\n",
    "print(\"\\nResponse Metadata:\")\n",
    "print(response_g.response_metadata)\n",
    "\n",
    "# Access specific metadata\n",
    "if 'token_usage' in response_g.response_metadata:\n",
    "    usage = response_g.response_metadata['token_usage']\n",
    "    print(f\"\\nTokens Used:\")\n",
    "    print(f\"  Prompt: {usage.get('prompt_tokens', 'N/A')}\")\n",
    "    print(f\"  Completion: {usage.get('completion_tokens', 'N/A')}\")\n",
    "    print(f\"  Total: {usage.get('total_tokens', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”° BEGINNER: Using Prompts\n",
    "\n",
    "Instead of plain strings, use **prompt templates** for better control:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Template:\n",
      "Explain {topic} in simple terms suitable for beginners.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Create a prompt template\n",
    "# {topic} is a variable we'll fill in later\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Explain {topic} in simple terms suitable for beginners.\"\n",
    ")\n",
    "\n",
    "# View the prompt structure\n",
    "print(\"Prompt Template:\")\n",
    "print(prompt.messages[0].prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StrOutputParser : Structure Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Topic: MACHINE LEARNING\n",
      "============================================================\n",
      "Machine learning is a type of technology that allows computers to learn and make decisions without being explicitly programmed. Instead of being told exactly what to do, the computer is given a set of data and algorithms to analyze that data and make predictions or decisions based on patterns it finds. Over time, the computer gets better at making these decisions as it learns from more data. It is like teaching a computer to think and make decisions on its own.\n",
      "\n",
      "============================================================\n",
      "Topic: EMBEDDINGS\n",
      "============================================================\n",
      "Embeddings are a way to represent words or objects as vectors in a multi-dimensional space. Each word or object is assigned a unique vector that captures its meaning and relationships with other words or objects. These vectors are learned through training on a large dataset, such as a corpus of text. By using embeddings, we can perform tasks like word similarity, sentiment analysis, and language translation more effectively because the vectors capture semantic relationships between words. In essence, embeddings help us understand the meaning of words and objects in a way that a computer can process and analyze.\n",
      "\n",
      "============================================================\n",
      "Topic: VECTOR DATABASES\n",
      "============================================================\n",
      "Vector databases are a type of database that store and manage data in the form of vectors. Vectors are mathematical objects that have both magnitude and direction, and in the context of databases, they are used to represent data points in multi-dimensional space.\n",
      "\n",
      "In simpler terms, think of a vector database as a way to store and organize information based on its location and characteristics in space. This can be useful for storing data such as geographic coordinates, customer preferences, or any other data that can be represented as a point in space.\n",
      "\n",
      "By using vectors to represent data, vector databases can efficiently store and retrieve information based on its spatial relationships, making them useful for applications such as geographic information systems, recommendation engines, and machine learning algorithms.\n"
     ]
    }
   ],
   "source": [
    "# Build a chain: Prompt â†’ LLM\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Components:\n",
    "# 1. prompt: Formats the input\n",
    "# 2. llm: Generates the response\n",
    "# 3. StrOutputParser: Extracts just the text from the response\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Use the chain with different topics\n",
    "topics = [\"machine learning\", \"embeddings\", \"vector databases\"]\n",
    "\n",
    "for topic in topics:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Topic: {topic.upper()}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Invoke the chain\n",
    "    explanation = chain.invoke({\"topic\": topic})\n",
    "    print(explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ INTERMEDIATE: Batch Processing\n",
    "\n",
    "Process multiple inputs efficiently using `.batch()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. RAG:\n",
      "   RAG stands for Red, Amber, Green. It is a simple system used to categorize and prioritize tasks or p...\n",
      "\n",
      "2. LCEL:\n",
      "   LCEL, or Low Cost Embedded Linux, is a type of operating system that is designed to run on small, lo...\n",
      "\n",
      "3. LANGCHAIN AGENTS:\n",
      "   LangChain agents are software programs that help users interact with the LangChain platform. These a...\n"
     ]
    }
   ],
   "source": [
    "# Batch process multiple topics at once\n",
    "topics_batch = [\n",
    "    {\"topic\": \"RAG\"},\n",
    "    {\"topic\": \"LCEL\"},\n",
    "    {\"topic\": \"LangChain agents\"}\n",
    "]\n",
    "\n",
    "# Batch processing is more efficient than calling invoke() multiple times\n",
    "results = chain.batch(topics_batch)\n",
    "\n",
    "# Print results\n",
    "for i, (input_dict, result) in enumerate(zip(topics_batch, results), 1):\n",
    "    print(f\"\\n{i}. {input_dict['topic'].upper()}:\")\n",
    "    print(f\"   {result[:100]}...\")  # Print first 100 chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"comparison\"></a>\n",
    "## 6. LangChain vs Traditional ML Pipelines ğŸ†š\n",
    "\n",
    "### ğŸ”° BEGINNER: Key Differences\n",
    "\n",
    "| Aspect | Traditional ML | LangChain |\n",
    "|--------|---------------|------------|\n",
    "| **Setup** | Complex, manual | Pre-built components |\n",
    "| **Integration** | Write custom code | Use existing integrations |\n",
    "| **Composability** | Difficult to chain | Easy with LCEL |\n",
    "| **Debugging** | Manual logging | Built-in callbacks |\n",
    "| **Prototyping** | Slow | Very fast |\n",
    "| **Code Reuse** | Limited | High |\n",
    "\n",
    "### Example: Building a Q&A System\n",
    "\n",
    "#### Without LangChain (50+ lines):\n",
    "```python\n",
    "import openai\n",
    "import PyPDF2\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load PDF manually\n",
    "def load_pdf(file_path):\n",
    "    reader = PyPDF2.PdfReader(file_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# 2. Split text manually\n",
    "def split_text(text, chunk_size=1000):\n",
    "    chunks = []\n",
    "    for i in range(0, len(text), chunk_size):\n",
    "        chunks.append(text[i:i+chunk_size])\n",
    "    return chunks\n",
    "\n",
    "# 3. Create embeddings manually\n",
    "def create_embeddings(chunks):\n",
    "    embeddings = []\n",
    "    for chunk in chunks:\n",
    "        response = openai.Embedding.create(\n",
    "            input=chunk,\n",
    "            model=\"text-embedding-ada-002\"\n",
    "        )\n",
    "        embeddings.append(response['data'][0]['embedding'])\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# 4. Create vector store manually\n",
    "def create_vector_store(embeddings):\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "# 5. Search manually\n",
    "def search(query, index, chunks):\n",
    "    query_embedding = openai.Embedding.create(\n",
    "        input=query,\n",
    "        model=\"text-embedding-ada-002\"\n",
    "    )['data'][0]['embedding']\n",
    "    \n",
    "    distances, indices = index.search(\n",
    "        np.array([query_embedding]), k=3\n",
    "    )\n",
    "    return [chunks[i] for i in indices[0]]\n",
    "\n",
    "# 6. Generate answer manually\n",
    "def generate_answer(query, context):\n",
    "    prompt = f\"Context: {context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "# Use it:\n",
    "text = load_pdf(\"document.pdf\")\n",
    "chunks = split_text(text)\n",
    "embeddings = create_embeddings(chunks)\n",
    "index = create_vector_store(embeddings)\n",
    "relevant_chunks = search(\"What is RAG?\", index, chunks)\n",
    "answer = generate_answer(\"What is RAG?\", \" \".join(relevant_chunks))\n",
    "```\n",
    "\n",
    "#### With LangChain (10 lines!):\n",
    "```python\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Load, split, embed, and store\n",
    "docs = PyPDFLoader(\"document.pdf\").load()\n",
    "chunks = RecursiveCharacterTextSplitter(chunk_size=1000).split_documents(docs)\n",
    "vectorstore = FAISS.from_documents(chunks, OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Build RAG chain\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    ")\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt | ChatOpenAI() | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Use it:\n",
    "answer = chain.invoke(\"What is RAG?\")\n",
    "```\n",
    "\n",
    "**50+ lines â†’ 10 lines!** ğŸ‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "## 7. Summary & Next Steps ğŸ“\n",
    "\n",
    "### ğŸ‰ What You Learned\n",
    "\n",
    "âœ… **LangChain** is a framework that simplifies building LLM applications\n",
    "\n",
    "âœ… **Architecture** is modular: loaders, splitters, embeddings, vector stores, chains\n",
    "\n",
    "âœ… **Documents** contain `page_content` (text) and `metadata` (information about the text)\n",
    "\n",
    "âœ… **LCEL** uses the `|` operator to chain components together\n",
    "\n",
    "âœ… **Chains** connect multiple components to create complex workflows\n",
    "\n",
    "âœ… LangChain **dramatically reduces code** compared to manual implementations\n",
    "\n",
    "### ğŸ”° For Beginners\n",
    "You now understand:\n",
    "- What LangChain is and why it's useful\n",
    "- How to set up your environment\n",
    "- Basic concepts: Documents and Chains\n",
    "- How to make your first LLM call\n",
    "\n",
    "### ğŸ“ For Intermediate Learners\n",
    "You now understand:\n",
    "- LangChain's package structure (1.0+ reorganization)\n",
    "- LCEL internals and advantages\n",
    "- Metadata usage for filtering and citation\n",
    "- Batch processing for efficiency\n",
    "\n",
    "### ğŸ“š Next Notebooks\n",
    "\n",
    "1. **Notebook 02**: Document Loaders (PDF, CSV, JSON, HTML)\n",
    "2. **Notebook 03**: Text Splitting Strategies\n",
    "3. **Notebook 04**: Embeddings and Vector Representations\n",
    "4. **Notebook 05**: Vector Stores (FAISS, Chroma)\n",
    "5. **Notebook 06**: Retrieval Strategies\n",
    "6. **Notebook 07**: Complete RAG Pipeline\n",
    "\n",
    "### ğŸ’¡ Practice Exercises\n",
    "\n",
    "Before moving to the next notebook, try these:\n",
    "\n",
    "1. **Easy**: Create 5 Documents about different topics with meaningful metadata\n",
    "2. **Medium**: Build a chain that takes a topic and generates a haiku about it\n",
    "3. **Advanced**: Create a chain that summarizes text in different styles (formal, casual, technical)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ Additional Resources\n",
    "\n",
    "- [Official LangChain Documentation](https://python.langchain.com/docs/)\n",
    "- [LCEL Guide](https://python.langchain.com/docs/expression_language/)\n",
    "- [LangChain GitHub](https://github.com/langchain-ai/langchain)\n",
    "\n",
    "---\n",
    "\n",
    "**Ready for more? Continue to Notebook 02: Document Loaders! ğŸš€**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simplerag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
