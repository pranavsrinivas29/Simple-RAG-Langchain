{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08 - External Index Retrievers üåê\n",
    "\n",
    "## Learning Objectives üéØ\n",
    "\n",
    "In this notebook, you'll learn:\n",
    "\n",
    "1. **What are External Index Retrievers** and how they differ from vector store retrievers\n",
    "2. **ArxivRetriever** - Search and retrieve scholarly articles from arxiv.org\n",
    "3. **WikipediaRetriever** - Access Wikipedia articles for general knowledge\n",
    "4. **TavilySearchAPIRetriever** - Perform real-time internet searches\n",
    "5. **Integration with RAG Chains** - Combine external retrievers with LLMs\n",
    "6. **Best Practices** - When and how to use each retriever effectively\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents üìö\n",
    "\n",
    "1. [Introduction to External Retrievers](#intro)\n",
    "2. [Setup & Installation](#setup)\n",
    "3. [ArxivRetriever - Academic Papers](#arxiv)\n",
    "4. [WikipediaRetriever - General Knowledge](#wikipedia)\n",
    "5. [TavilySearchAPIRetriever - Web Search](#tavily)\n",
    "6. [Integration with RAG Chains](#rag)\n",
    "7. [Comparison & Use Cases](#comparison)\n",
    "8. [Best Practices](#best-practices)\n",
    "9. [Summary & Exercises](#summary)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## 1. Introduction to External Index Retrievers üîç\n",
    "\n",
    "### What are External Index Retrievers?\n",
    "\n",
    "**External Index Retrievers** search over external data sources (e.g., the internet, academic databases, knowledge bases) rather than your local vector store.\n",
    "\n",
    "### Key Differences:\n",
    "\n",
    "| Feature | Vector Store Retrievers | External Index Retrievers |\n",
    "|---------|------------------------|---------------------------|\n",
    "| **Data Source** | Your embedded documents | External databases/APIs |\n",
    "| **Data Freshness** | Static (at indexing time) | Real-time or regularly updated |\n",
    "| **Setup Required** | Embedding + Vector store | API keys (sometimes) |\n",
    "| **Use Cases** | Internal documents, knowledge bases | Current events, academic research, general knowledge |\n",
    "| **Cost** | Embedding cost + storage | API calls (often free tier available) |\n",
    "\n",
    "### When to Use External Retrievers:\n",
    "\n",
    "- ‚úÖ You need **up-to-date information** from the internet\n",
    "- ‚úÖ You want to access **specialized databases** (e.g., academic papers)\n",
    "- ‚úÖ You need **general knowledge** without building a custom knowledge base\n",
    "- ‚úÖ You want to **augment** your local data with external sources\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='setup'></a>\n",
    "## 2. Setup & Installation ‚öôÔ∏è\n",
    "\n",
    "### Required Packages\n",
    "\n",
    "All external retrievers are part of `langchain-community`. You'll also need:\n",
    "\n",
    "```bash\n",
    "pip install langchain-community\n",
    "pip install arxiv           # For ArxivRetriever\n",
    "pip install wikipedia       # For WikipediaRetriever\n",
    "pip install tavily-python   # For TavilySearchAPIRetriever\n",
    "```\n",
    "\n",
    "### Environment Variables\n",
    "\n",
    "For TavilySearchAPIRetriever, you'll need an API key:\n",
    "\n",
    "```\n",
    "TAVILY_API_KEY=your_api_key_here\n",
    "```\n",
    "\n",
    "Get your free API key at: https://tavily.com/\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LangChain version: 1.0.5\n",
      "‚úÖ Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Setup: Import required libraries\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Import LangChain components\n",
    "from langchain_community.retrievers import ArxivRetriever, WikipediaRetriever, TavilySearchAPIRetriever\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Verify versions\n",
    "import langchain\n",
    "print(f\"‚úÖ LangChain version: {langchain.__version__}\")\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='arxiv'></a>\n",
    "## 3. ArxivRetriever - Academic Papers üìÑ\n",
    "\n",
    "### üî∞ BEGINNER: What is ArxivRetriever?\n",
    "\n",
    "**ArxivRetriever** searches [arxiv.org](https://arxiv.org), a repository of electronic preprints for research papers in:\n",
    "- Physics\n",
    "- Mathematics\n",
    "- Computer Science\n",
    "- Quantitative Biology\n",
    "- Quantitative Finance\n",
    "- Statistics\n",
    "\n",
    "### Use Cases:\n",
    "- üìö Literature review for research\n",
    "- üß† Getting latest research on AI/ML topics\n",
    "- üìä Finding papers by specific authors\n",
    "- üî¨ Accessing cutting-edge research\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî∞ BEGINNER: Basic ArxivRetriever Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.11.6 environment at: simplerag\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m8 packages\u001b[0m \u001b[2min 618ms\u001b[0m\u001b[0m                                         \u001b[0m\n",
      "\u001b[2K   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m sgmllib3k\u001b[2m==1.0.0\u001b[0m                                           \n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m sgmllib3k\u001b[2m==1.0.0\u001b[0m                                   \u001b[1A\n",
      "\u001b[37m‚†ô\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)\n",
      "\u001b[2K\u001b[2A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m sgmllib3k\u001b[2m==1.0.0\u001b[0m-----------\u001b[0m\u001b[0m     0 B/79.57 KiB           \u001b[2A\n",
      "\u001b[37m‚†ô\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)\n",
      "\u001b[2K\u001b[2A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m sgmllib3k\u001b[2m==1.0.0\u001b[0m-----------\u001b[0m\u001b[0m 16.00 KiB/79.57 KiB         \u001b[2A\n",
      "\u001b[37m‚†ô\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)\n",
      "\u001b[2K\u001b[2A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m sgmllib3k\u001b[2m==1.0.0\u001b[0m-----------\u001b[0m\u001b[0m 32.00 KiB/79.57 KiB         \u001b[2A\n",
      "\u001b[37m‚†ô\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)\n",
      "\u001b[2K\u001b[2A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m sgmllib3k\u001b[2m==1.0.0\u001b[0m-----------\u001b[0m\u001b[0m 48.00 KiB/79.57 KiB         \u001b[2A\n",
      "\u001b[37m‚†ô\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)\n",
      "\u001b[2K\u001b[2A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m sgmllib3k\u001b[2m==1.0.0\u001b[0mm\u001b[2m------\u001b[0m\u001b[0m 64.00 KiB/79.57 KiB         \u001b[2A\n",
      "\u001b[37m‚†ô\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)\n",
      "\u001b[2K\u001b[2A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m sgmllib3k\u001b[2m==1.0.0\u001b[0m--\u001b[30m\u001b[2m\u001b[0m\u001b[0m 79.57 KiB/79.57 KiB         \u001b[2A\n",
      "\u001b[37m‚†ô\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)\n",
      "\u001b[2K\u001b[2A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m sgmllib3k\u001b[2m==1.0.0\u001b[0m--\u001b[30m\u001b[2m\u001b[0m\u001b[0m 79.57 KiB/79.57 KiB         \u001b[2A\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m sgmllib3k\u001b[2m==1.0.0\u001b[0m                                   \u001b[1A\n",
      "\u001b[37m‚†ô\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)\n",
      "\u001b[2K\u001b[2A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m sgmllib3k\u001b[2m==1.0.0\u001b[0m-----------\u001b[0m\u001b[0m     0 B/11.29 KiB           \u001b[2A\n",
      "\u001b[37m‚†ô\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)\n",
      "\u001b[2K\u001b[2A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m sgmllib3k\u001b[2m==1.0.0\u001b[0m--\u001b[30m\u001b[2m\u001b[0m\u001b[0m 11.29 KiB/11.29 KiB         \u001b[2A\n",
      "\u001b[37m‚†ô\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/3)\n",
      "\u001b[2K\u001b[2A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m sgmllib3k\u001b[2m==1.0.0\u001b[0m--\u001b[30m\u001b[2m\u001b[0m\u001b[0m 11.29 KiB/11.29 KiB         \u001b[2A\n",
      "\u001b[2K\u001b[1A      \u001b[32m\u001b[1mBuilt\u001b[0m\u001b[39m sgmllib3k\u001b[2m==1.0.0\u001b[0m                                   \u001b[1A\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m3 packages\u001b[0m \u001b[2min 137ms\u001b[0m\u001b[0m                                             \n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m3 packages\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m                                 \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1marxiv\u001b[0m\u001b[2m==2.3.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfeedparser\u001b[0m\u001b[2m==6.0.12\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msgmllib3k\u001b[0m\u001b[2m==1.0.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Found 3 papers on 'large language models'\n",
      "\n",
      "================================================================================\n",
      "Title: Large Language Models Lack Understanding of Character Composition of Words\n",
      "Authors: Andrew Shin, Kunitake Kaneko\n",
      "Published: 2024-07-23\n",
      "\n",
      "Abstract (first 500 chars):\n",
      "Large language models (LLMs) have demonstrated remarkable performances on a wide range of natural language tasks. Yet, LLMs' successes have been largely restricted to tasks concerning words, sentences, or documents, and it remains questionable how much they understand the minimal units of text, namely characters. In this paper, we examine contemporary LLMs regarding their ability to understand character composition of words, and show that most of them fail to reliably carry out even the simple t...\n",
      "================================================================================\n",
      "Title: Is Self-knowledge and Action Consistent or Not: Investigating Large Language Model's Personality\n",
      "================================================================================\n",
      "Title: Unmasking the Shadows of AI: Investigating Deceptive Capabilities in Large Language Models\n"
     ]
    }
   ],
   "source": [
    "# Create an ArxivRetriever instance\n",
    "# By default, it returns top 3 documents\n",
    "arxiv_retriever = ArxivRetriever(load_max_docs=3)\n",
    "\n",
    "# Search for papers on \"large language models\"\n",
    "query = \"large language models\"\n",
    "docs = arxiv_retriever.invoke(query)\n",
    "\n",
    "print(f\"üìö Found {len(docs)} papers on '{query}'\\n\")\n",
    "\n",
    "# Display first paper\n",
    "print(\"=\" * 80)\n",
    "print(f\"Title: {docs[0].metadata.get('Title', 'N/A')}\")\n",
    "print(f\"Authors: {docs[0].metadata.get('Authors', 'N/A')}\")\n",
    "print(f\"Published: {docs[0].metadata.get('Published', 'N/A')}\")\n",
    "print(f\"\\nAbstract (first 500 chars):\\n{docs[0].page_content[:500]}...\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Title: {docs[1].metadata.get('Title', 'N/A')}\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Title: {docs[2].metadata.get('Title', 'N/A')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì INTERMEDIATE: Advanced ArxivRetriever Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Retrieved 3 papers\n",
      "\n",
      "1. Vision Transformer with Quadrangle Attention\n",
      "   Authors: Qiming Zhang, Jing Zhang, Yufei Xu, Dacheng Tao\n",
      "   Published: 2023-03-27\n",
      "   Entry ID: N/A\n",
      "\n",
      "2. D√©j√† vu: A Contextualized Temporal Attention Mechanism for Sequential Recommendation\n",
      "   Authors: Jibang Wu, Renqin Cai, Hongning Wang\n",
      "   Published: 2020-01-29\n",
      "   Entry ID: N/A\n",
      "\n",
      "3. Self-Attention as Distributional Projection: A Unified Interpretation of Transformer Architecture\n",
      "   Authors: Nihal Mehta\n",
      "   Published: 2025-11-16\n",
      "   Entry ID: N/A\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Advanced: Retrieve more documents and explore metadata\n",
    "arxiv_retriever_advanced = ArxivRetriever(\n",
    "    load_max_docs=5,  # Get top 5 papers\n",
    "    load_all_available_meta=True  # Load all metadata\n",
    ")\n",
    "\n",
    "# Search for papers on \"transformers attention mechanism\"\n",
    "query = \"transformers attention mechanism\"\n",
    "docs = arxiv_retriever_advanced.invoke(query)\n",
    "\n",
    "print(f\"üìö Retrieved {len(docs)} papers\\n\")\n",
    "\n",
    "# Display metadata for all papers\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    print(f\"{i}. {doc.metadata.get('Title', 'N/A')}\")\n",
    "    print(f\"   Authors: {doc.metadata.get('Authors', 'N/A')}\")\n",
    "    print(f\"   Published: {doc.metadata.get('Published', 'N/A')}\")\n",
    "    print(f\"   Entry ID: {doc.metadata.get('entry_id', 'N/A')}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì INTERMEDIATE: Using .batch() for Multiple Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Batch Search Results:\n",
      "\n",
      "Query: 'RAG retrieval augmented generation'\n",
      "  ‚Üí Found 3 papers\n",
      "  ‚Üí Top result: AR-RAG: Autoregressive Retrieval Augmentation for Image Generation\n",
      "\n",
      "Query: 'vector embeddings'\n",
      "  ‚Üí Found 3 papers\n",
      "  ‚Üí Top result: Part-of-Speech Relevance Weights for Learning Word Embeddings\n",
      "\n",
      "Query: 'prompt engineering'\n",
      "  ‚Üí Found 3 papers\n",
      "  ‚Üí Top result: Towards Goal-oriented Prompt Engineering for Large Language Models: A Survey\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Batch processing: Search multiple topics at once\n",
    "queries = [\n",
    "    \"RAG retrieval augmented generation\",\n",
    "    \"vector embeddings\",\n",
    "    \"prompt engineering\"\n",
    "]\n",
    "\n",
    "arxiv_retriever_batch = ArxivRetriever(load_max_docs=3)\n",
    "batch_results = arxiv_retriever_batch.batch(queries)\n",
    "\n",
    "print(\"üìö Batch Search Results:\\n\")\n",
    "for query, docs in zip(queries, batch_results):\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(f\"  ‚Üí Found {len(docs)} papers\")\n",
    "    if docs:\n",
    "        print(f\"  ‚Üí Top result: {docs[0].metadata.get('Title', 'N/A')}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Understanding ArxivRetriever Metadata\n",
    "\n",
    "Each document returned by ArxivRetriever contains rich metadata:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'Published': '2023-06-15',           # Publication date\n",
    "    'Title': 'Paper Title',              # Full title\n",
    "    'Authors': 'Author1, Author2',       # Comma-separated authors\n",
    "    'Summary': 'Abstract text...',       # Paper abstract/summary\n",
    "    'entry_id': 'http://arxiv.org/...',  # Arxiv URL\n",
    "}\n",
    "```\n",
    "\n",
    "The `page_content` field contains the full abstract/summary of the paper.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='wikipedia'></a>\n",
    "## 4. WikipediaRetriever - General Knowledge üìñ\n",
    "\n",
    "### üî∞ BEGINNER: What is WikipediaRetriever?\n",
    "\n",
    "**WikipediaRetriever** searches and retrieves content from Wikipedia, the free encyclopedia with 6+ million articles.\n",
    "\n",
    "### Use Cases:\n",
    "- üåç General knowledge questions\n",
    "- üìö Quick facts and definitions\n",
    "- üèõÔ∏è Historical information\n",
    "- üßë‚Äçüî¨ Biographical data\n",
    "- üó∫Ô∏è Geographic information\n",
    "\n",
    "### Important Notes:\n",
    "- ‚ö†Ô∏è Wikipedia content is **community-edited** - verify critical information\n",
    "- ‚úÖ Great for general knowledge, not for specialized or proprietary data\n",
    "- üåê Supports multiple languages\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî∞ BEGINNER: Basic WikipediaRetriever Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.11.6 environment at: simplerag\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m9 packages\u001b[0m \u001b[2min 560ms\u001b[0m\u001b[0m                                         \u001b[0m\n",
      "\u001b[2K   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m wikipedia\u001b[2m==1.4.0\u001b[0m                                           \n",
      "\u001b[2K\u001b[1A      \u001b[32m\u001b[1mBuilt\u001b[0m\u001b[39m wikipedia\u001b[2m==1.4.0\u001b[0m                                   \u001b[1A\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 137ms\u001b[0m\u001b[0m                                              \n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m                                  \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwikipedia\u001b[0m\u001b[2m==1.4.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Found 2 Wikipedia articles on 'Python programming language'\n",
      "\n",
      "================================================================================\n",
      "Title: Python (programming language)\n",
      "Source: https://en.wikipedia.org/wiki/Python_(programming_language)\n",
      "\n",
      "Content (first 600 chars):\n",
      "Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation. Python is dynamically type-checked and garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming.\n",
      "Guido van Rossum began working on Python in the late 1980s as a successor to the ABC programming language. Python 3.0, released in 2008, was a major revision and not completely backward-compatible with earlier versions. Beginning with Python 3.5, capabi...\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create a WikipediaRetriever instance\n",
    "# By default, it returns top 3 documents\n",
    "wiki_retriever = WikipediaRetriever(top_k_results=2)\n",
    "\n",
    "# Search for information on \"Python programming language\"\n",
    "query = \"Python programming language\"\n",
    "docs = wiki_retriever.invoke(query)\n",
    "\n",
    "print(f\"üìñ Found {len(docs)} Wikipedia articles on '{query}'\\n\")\n",
    "\n",
    "# Display first result\n",
    "print(\"=\" * 80)\n",
    "print(f\"Title: {docs[0].metadata.get('title', 'N/A')}\")\n",
    "print(f\"Source: {docs[0].metadata.get('source', 'N/A')}\")\n",
    "print(f\"\\nContent (first 600 chars):\\n{docs[0].page_content[:600]}...\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì INTERMEDIATE: Advanced WikipediaRetriever Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Retrieved 3 Wikipedia articles\n",
      "\n",
      "1. Title: Machine learning\n",
      "   Summary: Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn...\n",
      "   Content length: 1000 characters\n",
      "\n",
      "2. Title: Neural network (machine learning)\n",
      "   Summary: In machine learning, a neural network or neural net (NN), also called artificial neural network (ANN), is a computational model inspired by the struct...\n",
      "   Content length: 1000 characters\n",
      "\n",
      "3. Title: Quantum machine learning\n",
      "   Summary: Quantum machine learning (QML), pioneered by Ventura and Martinez  and by Trugenberger in the late 1990s and early 2000s, is the study of quantum algo...\n",
      "   Content length: 1000 characters\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Advanced: Control number of results and document length\n",
    "wiki_retriever_advanced = WikipediaRetriever(\n",
    "    top_k_results=3,        # Get top 3 results\n",
    "    doc_content_chars_max=1000  # Limit content to 1000 characters per doc\n",
    ")\n",
    "\n",
    "# Search for \"Machine Learning\"\n",
    "query = \"Machine Learning\"\n",
    "docs = wiki_retriever_advanced.invoke(query)\n",
    "\n",
    "print(f\"üìñ Retrieved {len(docs)} Wikipedia articles\\n\")\n",
    "\n",
    "# Display all results\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    print(f\"{i}. Title: {doc.metadata.get('title', 'N/A')}\")\n",
    "    print(f\"   Summary: {doc.metadata.get('summary', 'N/A')[:150]}...\")\n",
    "    print(f\"   Content length: {len(doc.page_content)} characters\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì INTERMEDIATE: Multilingual Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê Search in Spanish Wikipedia: 'Inteligencia Artificial'\n",
      "\n",
      "Title: Inteligencia artificial\n",
      "Content preview:\n",
      "La inteligencia artificial, abreviado como IA, en el contexto de las ciencias de la computaci√≥n, es una disciplina y un conjunto de capacidades cognoscitivas e intelectuales expresadas por sistemas inform√°ticos o combinaciones de algoritmos cuyo prop√≥sito es la creaci√≥n de m√°quinas que imiten la inteligencia humana.\n",
      "Estas tecnolog√≠as permiten que las m√°quinas aprendan de la experiencia, se adapten...\n"
     ]
    }
   ],
   "source": [
    "# Search in different languages\n",
    "# Default is English ('en'), but you can specify other languages\n",
    "\n",
    "# Example: Search in Spanish\n",
    "wiki_retriever_es = WikipediaRetriever(\n",
    "    top_k_results=1,\n",
    "    lang=\"es\"  # Spanish Wikipedia\n",
    ")\n",
    "\n",
    "query = \"Inteligencia Artificial\"\n",
    "docs = wiki_retriever_es.invoke(query)\n",
    "\n",
    "print(f\"üåê Search in Spanish Wikipedia: '{query}'\\n\")\n",
    "print(f\"Title: {docs[0].metadata.get('title', 'N/A')}\")\n",
    "print(f\"Content preview:\\n{docs[0].page_content[:400]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì INTERMEDIATE: Batch Processing with WikipediaRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Batch Wikipedia Search Results:\n",
      "\n",
      "Query: 'Albert Einstein'\n",
      "  ‚Üí Title: Albert Einstein\n",
      "  ‚Üí Summary: Albert Einstein (14 March 1879 ‚Äì 18 April 1955) was a German-born theoretical physicist best known for developing the theory of relativity. Einstein also made important contributions to quantum theory...\n",
      "\n",
      "Query: 'Quantum Computing'\n",
      "  ‚Üí Title: Quantum computing\n",
      "  ‚Üí Summary: A quantum computer is a (real or theoretical) computer that exploits superposed and entangled states, and the intrinsically non-deterministic outcomes of quantum measurements, as features of its compu...\n",
      "\n",
      "Query: 'Neural Networks'\n",
      "  ‚Üí Title: Neural network (machine learning)\n",
      "  ‚Üí Summary: In machine learning, a neural network or neural net (NN), also called artificial neural network (ANN), is a computational model inspired by the structure and functions of biological neural networks.\n",
      "A...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Batch search for multiple topics\n",
    "queries = [\n",
    "    \"Albert Einstein\",\n",
    "    \"Quantum Computing\",\n",
    "    \"Neural Networks\"\n",
    "]\n",
    "\n",
    "wiki_retriever_batch = WikipediaRetriever(top_k_results=1, doc_content_chars_max=500)\n",
    "batch_results = wiki_retriever_batch.batch(queries)\n",
    "\n",
    "print(\"üìñ Batch Wikipedia Search Results:\\n\")\n",
    "for query, docs in zip(queries, batch_results):\n",
    "    print(f\"Query: '{query}'\")\n",
    "    if docs:\n",
    "        print(f\"  ‚Üí Title: {docs[0].metadata.get('title', 'N/A')}\")\n",
    "        print(f\"  ‚Üí Summary: {docs[0].page_content[:200]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Understanding WikipediaRetriever Metadata\n",
    "\n",
    "Each document returned by WikipediaRetriever contains:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'title': 'Article Title',           # Wikipedia article title\n",
    "    'summary': 'Brief summary...',       # Short summary (if available)\n",
    "    'source': 'https://en.wikipedia...', # Full Wikipedia URL\n",
    "}\n",
    "```\n",
    "\n",
    "The `page_content` field contains the article text (up to `doc_content_chars_max` characters).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tavily'></a>\n",
    "## 5. TavilySearchAPIRetriever - Web Search üîç\n",
    "\n",
    "### üî∞ BEGINNER: What is TavilySearchAPIRetriever?\n",
    "\n",
    "**TavilySearchAPIRetriever** performs **real-time internet searches** using the Tavily Search API, optimized for AI applications.\n",
    "\n",
    "### Key Features:\n",
    "- üåê **Real-time web search** - Get the latest information from the internet\n",
    "- üéØ **AI-optimized** - Returns clean, relevant content for LLMs\n",
    "- üîí **Source attribution** - Includes URLs and metadata\n",
    "- ‚ö° **Fast & reliable** - Built specifically for AI use cases\n",
    "\n",
    "### Use Cases:\n",
    "- üì∞ Current events and news\n",
    "- üíπ Stock prices and market data\n",
    "- üå¶Ô∏è Weather information\n",
    "- üè¢ Company information\n",
    "- üîß Technical documentation and tutorials\n",
    "\n",
    "### Getting Started:\n",
    "1. Sign up at https://tavily.com/ (free tier available)\n",
    "2. Get your API key\n",
    "3. Add to `.env` file: `TAVILY_API_KEY=your_api_key`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî∞ BEGINNER: Basic TavilySearchAPIRetriever Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.11.6 environment at: simplerag\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m14 packages\u001b[0m \u001b[2min 254ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[37m‚†ô\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)                                                   \n",
      "\u001b[2K\u001b[1A\u001b[37m‚†ô\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)-------------------\u001b[0m\u001b[0m     0 B/15.12 KiB           \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m‚†ô\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)---------\u001b[30m\u001b[2m-\u001b[0m\u001b[0m 14.85 KiB/15.12 KiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m‚†ô\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)----------\u001b[30m\u001b[2m\u001b[0m\u001b[0m 15.12 KiB/15.12 KiB         \u001b[1A\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 28ms\u001b[0m\u001b[0m                                                        \u001b[1A\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m13                                \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtavily-python\u001b[0m\u001b[2m==0.7.13\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Found 3 web results for 'latest developments in artificial intelligence 2024'\n",
      "\n",
      "================================================================================\n",
      "Source: https://www.launchconsulting.com/posts/the-future-of-business-ai-innovations-to-watch-in-2024\n",
      "\n",
      "Content (first 500 chars):\n",
      "Key AI trends for 2024 include generative AI beyond chatbots, small language models, multi-modal experiences, AI for professionals, and evolving legislation....\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create a TavilySearchAPIRetriever instance\n",
    "# Make sure TAVILY_API_KEY is set in your .env file\n",
    "\n",
    "tavily_retriever = TavilySearchAPIRetriever(k=3)  # Return top 3 results\n",
    "\n",
    "# Search for \"latest developments in artificial intelligence 2024\"\n",
    "query = \"latest developments in artificial intelligence 2024\"\n",
    "docs = tavily_retriever.invoke(query)\n",
    "\n",
    "print(f\"üîç Found {len(docs)} web results for '{query}'\\n\")\n",
    "\n",
    "# Display first result\n",
    "print(\"=\" * 80)\n",
    "print(f\"Source: {docs[0].metadata.get('source', 'N/A')}\")\n",
    "print(f\"\\nContent (first 500 chars):\\n{docs[0].page_content[:500]}...\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì INTERMEDIATE: Advanced TavilySearchAPIRetriever Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Retrieved 5 web results\n",
      "\n",
      "1. Source: https://www.datacamp.com/tutorial/how-to-build-llm-applications-with-langchain\n",
      "   Content preview: This tutorial is perfect for you. Here, we explore LangChain - An open-source Python framework for building applications based on Large Language Models such as...\n",
      "\n",
      "2. Source: https://medium.com/data-science-in-your-pocket/langchain-tutorials-for-newbies-945319df04e2\n",
      "   Content preview: ## LangChain in your Pocket: Beginner's Guide to Building Generative AI Applications using LLMs ### LangChain in your Pocket: Beginner's Guide to Building Generative AI Applications using LLMs eBook :...\n",
      "\n",
      "3. Source: https://docs.langchain.com/oss/python/learn\n",
      "   Content preview: [LangChain](/oss/python/langchain/overview)[LangGraph](/oss/python/langgraph/overview)[Deep Agents](/oss/python/deepagents/overview)[Integrations](/oss/python/integrations/providers/overview)[Learn](/...\n",
      "\n",
      "4. Source: https://github.com/gkamradt/langchain-tutorials\n",
      "   Content preview: 2. LangChain CookBook Part 2: 9 Use Cases - Code, Video | Kor | Eugene Yurtsev | üêí Intermediate | ‚úÖ Code | This is a half-baked prototype that ‚Äúhelps‚Äù you extract structured data from text using large...\n",
      "\n",
      "5. Source: https://www.geeksforgeeks.org/artificial-intelligence/introduction-to-langchain/\n",
      "   Content preview: Vector database plays a key role in tasks like document retrieval, knowledge base integration or context-based search providing the model with dynamic, real-time data to enhance responses. LangChain f...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Advanced: Control search depth and domain filtering\n",
    "from langchain_community.retrievers import TavilySearchAPIRetriever\n",
    "\n",
    "# Advanced configuration\n",
    "tavily_retriever_advanced = TavilySearchAPIRetriever(\n",
    "    k=5,  # Return top 5 results\n",
    "    # search_depth=\"advanced\",  # \"basic\" or \"advanced\" (more thorough)\n",
    "    # include_domains=[\"github.com\", \"stackoverflow.com\"],  # Filter to specific domains\n",
    "    # exclude_domains=[\"example.com\"]  # Exclude specific domains\n",
    ")\n",
    "\n",
    "# Search for \"LangChain tutorials\"\n",
    "query = \"LangChain tutorials\"\n",
    "docs = tavily_retriever_advanced.invoke(query)\n",
    "\n",
    "print(f\"üîç Retrieved {len(docs)} web results\\n\")\n",
    "\n",
    "# Display all results with sources\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    print(f\"{i}. Source: {doc.metadata.get('source', 'N/A')}\")\n",
    "    print(f\"   Content preview: {doc.page_content[:200]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì INTERMEDIATE: Real-Time Information Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üïê Real-Time Information (as of November 24, 2025):\n",
      "\n",
      "Query: 'latest AI news November 24, 2025'\n",
      "  ‚Üí Nov 24, 2025, 03:45am EST. Contraction Before Expansion: How AI is Poised Spark Tech Services Growth. getty. Over the last two years, something unusual has...\n",
      "  ‚Üí Source: https://www.forbes.com/sites/peterbendorsamuel/2025/11/24/contraction-before-expansion-how-ai-is-poised-spark-tech-services-growth/\n",
      "\n",
      "Query: 'current weather in San Francisco'\n",
      "  ‚Üí {'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.775, 'lon': -122.4183, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1763982148, 'localtime': '2025-11-24 03:02'}, 'current': {'last_...\n",
      "  ‚Üí Source: https://www.weatherapi.com/\n",
      "\n",
      "Query: 'NVIDIA stock price today'\n",
      "  ‚Üí The NVIDIA Corporation Common Stock (NVDA) stock price today is $180.03, reflecting a -0.49% move since the market opened. The company's market capitalization...\n",
      "  ‚Üí Source: https://www.kraken.com/stocks/nvda\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: Get current information (news, weather, stock prices, etc.)\n",
    "from datetime import datetime\n",
    "\n",
    "current_date = datetime.now().strftime(\"%B %d, %Y\")\n",
    "\n",
    "# Real-time queries\n",
    "queries = [\n",
    "    f\"latest AI news {current_date}\",\n",
    "    \"current weather in San Francisco\",\n",
    "    \"NVIDIA stock price today\"\n",
    "]\n",
    "\n",
    "tavily_realtime = TavilySearchAPIRetriever(k=2)\n",
    "\n",
    "print(f\"üïê Real-Time Information (as of {current_date}):\\n\")\n",
    "\n",
    "for query in queries:\n",
    "    docs = tavily_realtime.invoke(query)\n",
    "    print(f\"Query: '{query}'\")\n",
    "    if docs:\n",
    "        print(f\"  ‚Üí {docs[0].page_content[:250]}...\")\n",
    "        print(f\"  ‚Üí Source: {docs[0].metadata.get('source', 'N/A')}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Understanding TavilySearchAPIRetriever Metadata\n",
    "\n",
    "Each document returned by TavilySearchAPIRetriever contains:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'source': 'https://example.com/...',  # Source URL\n",
    "    'score': 0.95,                         # Relevance score (0-1)\n",
    "    'title': 'Page Title',                 # Web page title (if available)\n",
    "}\n",
    "```\n",
    "\n",
    "The `page_content` field contains the extracted text content from the web page.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rag'></a>\n",
    "## 6. Integration with RAG Chains üîó\n",
    "\n",
    "Now let's combine external retrievers with LLMs to build powerful **Retrieval-Augmented Generation (RAG)** systems!\n",
    "\n",
    "### üî∞ BEGINNER: Simple QA Chain with External Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is quantum computing and how does it work?\n",
      "\n",
      "Answer: Quantum computing is a form of computing that uses quantum bits, or qubits, which can be in a superposition of 0 and 1 and can become entangled with one another. A quantum computer works by preparing qubits, applying quantum operations (gates) to them, and then measuring the qubits. Measurements are probabilistic: you get 0 or 1 according to quantum probabilities. By carefully manipulating the qubits, wave interference can amplify the probability of the desired outcome.\n",
      "\n",
      "Because a system of qubits can represent many possible states at once, a quantum computer can explore a large space of possibilities in parallel and sample from it, though it still has computational constraints. Algorithms are designed to construct procedures that increase the likelihood of the correct result when measured.\n",
      "\n",
      "As of now, quantum computers are not yet practical for general use; current hardware is largely experimental and suitable only for specialized tasks. In principle, quantum computing could break some widely used public-key cryptographic schemes (for example via Shor‚Äôs algorithm). By contrast, most symmetric cryptography and hash functions are relatively more resistant‚Äîthough Grover‚Äôs algorithm means larger key sizes are needed to maintain security.\n"
     ]
    }
   ],
   "source": [
    "# Build a simple RAG chain using WikipediaRetriever\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Initialize components\n",
    "wiki_retriever = WikipediaRetriever(top_k_results=2, doc_content_chars_max=2000)\n",
    "llm = ChatOpenAI(model=\"gpt-5-nano\", temperature=0)\n",
    "\n",
    "# Create prompt template\n",
    "template = \"\"\"Answer the question based on the following context from Wikipedia:\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Helper function to format documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Build the RAG chain using LCEL\n",
    "rag_chain = (\n",
    "    {\"context\": wiki_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Ask a question\n",
    "question = \"What is quantum computing and how does it work?\"\n",
    "answer = rag_chain.invoke(question)\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì INTERMEDIATE: Multi-Source RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are transformers in machine learning?\n",
      "\n",
      "Answer (from multiple sources):\n",
      "A transformer is a type of neural network architecture designed to model relationships in sequences using attention mechanisms, rather than relying on recurrence (like RNNs) or fixed convolutional windows. It is built around the idea of computing how much each part of a input sequence should influence every other part, and it does this using multi-head self-attention over token representations.\n",
      "\n",
      "What ‚Äútransformers‚Äù do and what they rely on\n",
      "- Tokens and embeddings: The input data are first converted into tokens, and each token is mapped to a vector through an embedding table. This turns discrete data (like words) into continuous vectors that the network can manipulate. (Wikipedia description)\n",
      "- Contextualization with multi-head attention: At each layer, every token is contextualized with respect to the rest of the tokens in the input via multi-head attention. This means the model learns which other tokens are most relevant to each token, and aggregates information accordingly, enabling context-aware representations. (Wikipedia description)\n",
      "- Architecture patterns: The original transformer paper introduced the encoder‚Äìdecoder structure, stacking layers that perform attention-based processing followed by feed-forward transformations. This structure has become foundational for large-scale language models. (Described in the context of the transformer‚Äôs impact and in the original work‚Äôs framing)\n",
      "- Key idea: ‚ÄúAttention is all you need‚Äù ‚Äî the architecture relies on attention mechanisms to capture dependencies across the sequence, enabling parallel processing of sequence elements and effective modeling of long-range relationships. The 2017 paper from Google researchers is the landmark that introduced this approach. (Attention Is All You Need, cited in general knowledge)\n",
      "\n",
      "What makes transformers distinctive\n",
      "- Parallelizable training: Because they compute attention across the entire sequence in each layer, transformers can be trained more efficiently on modern hardware than strictly sequential models.\n",
      "- Long-range dependencies: The self-attention mechanism allows the model to weigh and integrate information from distant parts of the sequence, which helps with tasks where context from far apart elements is important.\n",
      "- Broad applicability (at least initially in NLP): The transformer architecture became the mainstay for large language models and a wide range of AI tasks that involve sequential or structured data. (General knowledge overview)\n",
      "\n",
      "How transformers are used in practice\n",
      "- Input processing: Text or other sequential data are tokenized, transformed into embeddings, and then processed through stacked attention and feed-forward layers. The resulting representations are used to produce predictions, such as translated text, masked language modeling outputs, or next-token predictions in language models. (General description from the transformer overview)\n",
      "- Model design variations: There are encoder-only configurations (e.g., for classification), decoder-only configurations (e.g., for autoregressive generation like chat models), and encoder-decoder configurations (e.g., for translation). The original transformer paper and subsequent work laid out these structural options, with the encoder‚Äìdecoder pattern being central to many sequence-to-sequence tasks. (General context from the transformer literature)\n",
      "\n",
      "How this relates to broader ML practice (from the provided sources)\n",
      "- Reliability and validation in ML systems: Beyond the model architecture itself, the quality and integrity of data sources, as well as rigorous validation practices, are emphasized in ML research and practice. Calls for standardized validation approaches (e.g., DOME: data, optimization, model, evaluation) highlight that sophisticated models like transformers still require careful data handling and evaluation to ensure reliable, trustworthy results. (ArXiv excerpt on data science in statistics and calls for ML validation)\n",
      "- Embedding-based processing and dense models: The arXiv material shows how modern pipelines leverage embedding-based representations and dense models to process complex, high-volume data (e.g., continuous spatiotemporal data from event-based sensors). This illustrates a broader trend in ML toward rich, learned representations (embeddings) and powerful nonlinear processors, a category that includes transformer-style architectures for sequence data. (ArXiv excerpt on embeddings, ALERT module, and dense processing)\n",
      "\n",
      "In short\n",
      "- A transformer is a neural network architecture that uses self-attention (particularly multi-head attention) to model relationships within a sequence of tokens, with embeddings providing the initial numeric representations and positional information helping to preserve order.\n",
      "- It can be organized as encoders, decoders, or encoder‚Äìdecoder stacks, and is trained to perform a wide range of sequence modeling tasks, most famously enabling large language models.\n",
      "- Its rise is tied to the attention mechanism‚Äôs ability to capture global dependencies efficiently, enabling fast, scalable learning on large datasets.\n",
      "- In the broader ML landscape, rigorous data quality and validation remain crucial when applying transformers, just as with any powerful machine learning model.\n"
     ]
    }
   ],
   "source": [
    "# Advanced: Combine multiple retrievers for comprehensive answers\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "# Initialize multiple retrievers\n",
    "arxiv_retriever = ArxivRetriever(load_max_docs=2)\n",
    "wiki_retriever = WikipediaRetriever(top_k_results=2, doc_content_chars_max=1500)\n",
    "\n",
    "# Function to combine results from multiple retrievers\n",
    "def multi_retriever(query):\n",
    "    \"\"\"Retrieve from multiple sources and combine results.\"\"\"\n",
    "    arxiv_docs = arxiv_retriever.invoke(query)\n",
    "    wiki_docs = wiki_retriever.invoke(query)\n",
    "    \n",
    "    # Combine and format\n",
    "    all_docs = []\n",
    "    \n",
    "    if arxiv_docs:\n",
    "        all_docs.append(\"=== Academic Papers (ArXiv) ===\")\n",
    "        all_docs.extend([doc.page_content[:500] for doc in arxiv_docs])\n",
    "    \n",
    "    if wiki_docs:\n",
    "        all_docs.append(\"\\n=== General Knowledge (Wikipedia) ===\")\n",
    "        all_docs.extend([doc.page_content[:500] for doc in wiki_docs])\n",
    "    \n",
    "    return \"\\n\\n\".join(all_docs)\n",
    "\n",
    "# Create multi-source RAG chain\n",
    "multi_source_template = \"\"\"Answer the question using information from multiple sources below:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Provide a comprehensive answer that synthesizes information from both academic and general sources:\"\"\"\n",
    "\n",
    "multi_prompt = ChatPromptTemplate.from_template(multi_source_template)\n",
    "\n",
    "multi_rag_chain = (\n",
    "    {\"context\": multi_retriever, \"question\": RunnablePassthrough()}\n",
    "    | multi_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Ask a question\n",
    "question = \"What are transformers in machine learning?\"\n",
    "answer = multi_rag_chain.invoke(question)\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(f\"Answer (from multiple sources):\\n{answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì INTERMEDIATE: Real-Time RAG with TavilySearchAPIRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are the latest developments in AI regulation?\n",
      "\n",
      "Answer (from real-time web search):\n",
      "As of the latest developments in AI regulation, countries around the world are increasingly recognizing the need for comprehensive laws and guidelines to govern the use of artificial intelligence. The European Union's AI Act, which was proposed in April 2021 and is set to come into effect on August 1, 2024, is one of the most significant pieces of AI regulation to date. This regulation aims to establish harmonized rules for AI across all 27 EU member states, addressing issues such as transparency, accountability, and data governance.\n",
      "\n",
      "One of the key provisions of the EU AI Act is the creation of a regulatory framework for high-risk AI applications. These are defined as AI systems that pose significant risks to the health, safety, or fundamental rights of individuals. Examples of high-risk AI applications include autonomous vehicles, facial recognition technology, and predictive policing systems. Under the EU AI Act, developers of high-risk AI systems will be required to comply with strict requirements, such as conducting risk assessments, ensuring transparency and accountability, and implementing appropriate safeguards.\n",
      "\n",
      "In addition to the EU's efforts, the United States has also taken steps to regulate AI. In January 2021, former President Donald Trump signed an Executive Order on Removing Barriers to AI Leadership, which aimed to promote the development and use of AI in the United States. The executive order called for federal agencies to prioritize AI research and development, enhance AI education and training programs, and support the responsible use of AI technologies.\n",
      "\n",
      "Furthermore, China has been actively working on AI regulation, with a focus on both targeted laws and broader policies. China's approach to AI regulation is influenced by its economic and science and technology goals, as the country aims to become a global leader in AI innovation. In recent years, China has introduced laws and guidelines to govern specific AI applications, such as autonomous vehicles and facial recognition technology. Additionally, the Chinese government has issued national strategies and plans to promote the development of AI and ensure its responsible use.\n",
      "\n",
      "Overall, the latest developments in AI regulation reflect a growing recognition of the need to address the ethical, legal, and societal implications of artificial intelligence. As AI technologies continue to advance and become more integrated into various aspects of society, it is crucial for governments to establish clear rules and guidelines to ensure that AI is developed and used in a responsible and ethical manner. By implementing comprehensive AI regulations, countries can promote innovation, protect individual rights, and build trust in AI systems.\n"
     ]
    }
   ],
   "source": [
    "# Build a RAG chain that uses real-time web search\n",
    "tavily_retriever = TavilySearchAPIRetriever(k=3)\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Create prompt for real-time information\n",
    "realtime_template = \"\"\"Based on the latest information from the web:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Provide an up-to-date answer having atleast 500 words with source attribution:\"\"\"\n",
    "\n",
    "realtime_prompt = ChatPromptTemplate.from_template(realtime_template)\n",
    "\n",
    "# Build real-time RAG chain\n",
    "realtime_rag_chain = (\n",
    "    {\"context\": tavily_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | realtime_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Ask a current events question\n",
    "question = \"What are the latest developments in AI regulation?\"\n",
    "answer = realtime_rag_chain.invoke(question)\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(f\"Answer (from real-time web search):\\n{answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='comparison'></a>\n",
    "## 7. Comparison & Use Cases üìä\n",
    "\n",
    "### Retriever Comparison Table\n",
    "\n",
    "| Feature | ArxivRetriever | WikipediaRetriever | TavilySearchAPIRetriever |\n",
    "|---------|----------------|-------------------|-------------------------|\n",
    "| **Data Source** | Academic papers (arxiv.org) | Wikipedia articles | Real-time web search |\n",
    "| **API Key Required** | ‚ùå No | ‚ùå No | ‚úÖ Yes (free tier) |\n",
    "| **Data Freshness** | Recent research | Regularly updated | Real-time |\n",
    "| **Best For** | Academic research, ML papers | General knowledge, definitions | Current events, news |\n",
    "| **Content Type** | Research papers, abstracts | Encyclopedia articles | Web pages, news |\n",
    "| **Default Results** | 3 papers | 3 articles | 5 results |\n",
    "| **Multilingual** | ‚ùå No | ‚úÖ Yes (300+ languages) | ‚úÖ Yes |\n",
    "| **Metadata** | Title, Authors, Published date | Title, Summary, URL | Source URL, Score |\n",
    "| **Rate Limits** | Moderate | Moderate | API-dependent |\n",
    "| **Cost** | üÜì Free | üÜì Free | üÜì Free tier + paid |\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use Each Retriever\n",
    "\n",
    "#### ‚úÖ Use **ArxivRetriever** when:\n",
    "- You need peer-reviewed academic research\n",
    "- You're building an AI/ML research assistant\n",
    "- You want the latest scientific papers\n",
    "- You need citations and author information\n",
    "\n",
    "#### ‚úÖ Use **WikipediaRetriever** when:\n",
    "- You need general knowledge and definitions\n",
    "- You want historical or biographical information\n",
    "- You're building an educational chatbot\n",
    "- You need multilingual support\n",
    "- You want reliable, community-edited content\n",
    "\n",
    "#### ‚úÖ Use **TavilySearchAPIRetriever** when:\n",
    "- You need real-time, up-to-date information\n",
    "- You're answering current events questions\n",
    "- You want to search the broader internet\n",
    "- You need to filter by specific domains\n",
    "- Your use case requires the latest data\n",
    "\n",
    "---\n",
    "\n",
    "### Combining Retrievers (Hybrid Approach)\n",
    "\n",
    "For the most comprehensive RAG system:\n",
    "\n",
    "```python\n",
    "# Pseudo-code for hybrid retrieval\n",
    "if query_type == \"academic\":\n",
    "    use ArxivRetriever\n",
    "elif query_type == \"general_knowledge\":\n",
    "    use WikipediaRetriever\n",
    "elif query_type == \"current_events\":\n",
    "    use TavilySearchAPIRetriever\n",
    "else:\n",
    "    # Use multiple retrievers and combine results\n",
    "    combine(ArxivRetriever, WikipediaRetriever, TavilySearchAPIRetriever)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='best-practices'></a>\n",
    "## 8. Best Practices üí°\n",
    "\n",
    "### General Best Practices\n",
    "\n",
    "#### 1. **Handle Errors Gracefully**\n",
    "\n",
    "```python\n",
    "try:\n",
    "    docs = retriever.invoke(query)\n",
    "except Exception as e:\n",
    "    print(f\"Error retrieving documents: {e}\")\n",
    "    docs = []  # Fallback to empty list\n",
    "```\n",
    "\n",
    "#### 2. **Set Appropriate Limits**\n",
    "\n",
    "```python\n",
    "# Don't retrieve too many documents (costs, latency)\n",
    "arxiv_retriever = ArxivRetriever(load_max_docs=3)  # ‚úÖ Good\n",
    "arxiv_retriever = ArxivRetriever(load_max_docs=100)  # ‚ùå Too many\n",
    "```\n",
    "\n",
    "#### 3. **Cache Results for Repeated Queries**\n",
    "\n",
    "```python\n",
    "# Use a simple cache to avoid redundant API calls\n",
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=100)\n",
    "def cached_search(query: str):\n",
    "    return retriever.invoke(query)\n",
    "```\n",
    "\n",
    "#### 4. **Verify Source Attribution**\n",
    "\n",
    "```python\n",
    "# Always include sources in your responses\n",
    "for doc in docs:\n",
    "    print(f\"Source: {doc.metadata.get('source', 'N/A')}\")\n",
    "```\n",
    "\n",
    "#### 5. **Combine with Vector Store Retrievers**\n",
    "\n",
    "```python\n",
    "# Use external retrievers for general knowledge\n",
    "# Use vector stores for your proprietary data\n",
    "def hybrid_retrieve(query):\n",
    "    external_docs = wiki_retriever.invoke(query)\n",
    "    internal_docs = vector_store.similarity_search(query)\n",
    "    return external_docs + internal_docs\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Retriever-Specific Best Practices\n",
    "\n",
    "#### ArxivRetriever:\n",
    "- ‚úÖ Use specific search terms (e.g., \"BERT transformers\" vs \"AI\")\n",
    "- ‚úÖ Limit results to 3-5 papers for LLM context\n",
    "- ‚úÖ Extract metadata for citations\n",
    "- ‚ùå Don't use for non-academic queries\n",
    "\n",
    "#### WikipediaRetriever:\n",
    "- ‚úÖ Use for general knowledge, not specialized topics\n",
    "- ‚úÖ Set `doc_content_chars_max` to avoid huge documents\n",
    "- ‚úÖ Verify information for critical use cases\n",
    "- ‚ùå Don't rely on Wikipedia for real-time information\n",
    "\n",
    "#### TavilySearchAPIRetriever:\n",
    "- ‚úÖ Monitor API usage (rate limits, costs)\n",
    "- ‚úÖ Use for time-sensitive queries\n",
    "- ‚úÖ Filter by domain for specific sources\n",
    "- ‚ùå Don't use for queries that don't need real-time data\n",
    "\n",
    "---\n",
    "\n",
    "### Performance Tips\n",
    "\n",
    "1. **Use `.batch()` for multiple queries**\n",
    "   ```python\n",
    "   # ‚úÖ Efficient\n",
    "   results = retriever.batch([q1, q2, q3])\n",
    "   \n",
    "   # ‚ùå Inefficient\n",
    "   results = [retriever.invoke(q) for q in [q1, q2, q3]]\n",
    "   ```\n",
    "\n",
    "2. **Limit document length for LLM context**\n",
    "   ```python\n",
    "   # Truncate long documents to fit LLM context window\n",
    "   docs = [Document(page_content=doc.page_content[:2000], metadata=doc.metadata) \n",
    "           for doc in raw_docs]\n",
    "   ```\n",
    "\n",
    "3. **Use async methods for concurrent retrieval** (if supported)\n",
    "   ```python\n",
    "   # For async-compatible retrievers\n",
    "   import asyncio\n",
    "   docs = await retriever.ainvoke(query)\n",
    "   ```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='summary'></a>\n",
    "## 9. Summary & Exercises üìù\n",
    "\n",
    "### üéØ What You Learned\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "‚úÖ **External Index Retrievers** - Search over external data sources (internet, databases)\n",
    "\n",
    "‚úÖ **ArxivRetriever** - Retrieve academic papers from arxiv.org\n",
    "   - Use cases: Research, ML papers, citations\n",
    "   - Methods: `.invoke()`, `.batch()`\n",
    "   - Metadata: Title, Authors, Published date\n",
    "\n",
    "‚úÖ **WikipediaRetriever** - Access Wikipedia articles\n",
    "   - Use cases: General knowledge, definitions, history\n",
    "   - Features: Multilingual support, customizable length\n",
    "   - Metadata: Title, Summary, Source URL\n",
    "\n",
    "‚úÖ **TavilySearchAPIRetriever** - Real-time web search\n",
    "   - Use cases: Current events, news, real-time data\n",
    "   - Features: Domain filtering, search depth control\n",
    "   - Metadata: Source URL, Relevance score\n",
    "\n",
    "‚úÖ **RAG Integration** - Combined external retrievers with LLMs\n",
    "   - Built simple QA chains\n",
    "   - Created multi-source RAG systems\n",
    "   - Implemented real-time information retrieval\n",
    "\n",
    "‚úÖ **Best Practices** - Error handling, caching, source attribution\n",
    "\n",
    "---\n",
    "\n",
    "### üí™ Practice Exercises\n",
    "\n",
    "#### Exercise 1: Academic Research Assistant (üî∞ Beginner)\n",
    "Create a RAG chain that:\n",
    "- Uses `ArxivRetriever` to find papers on \"deep learning\"\n",
    "- Extracts the top 3 paper titles and authors\n",
    "- Summarizes each paper's abstract using an LLM\n",
    "\n",
    "#### Exercise 2: Wikipedia Fact Checker (üî∞ Beginner)\n",
    "Build a system that:\n",
    "- Takes a statement as input (e.g., \"Python was created in 1991\")\n",
    "- Uses `WikipediaRetriever` to search for relevant articles\n",
    "- Uses an LLM to verify if the statement is accurate\n",
    "\n",
    "#### Exercise 3: Multi-Source News Aggregator (üéì Intermediate)\n",
    "Create a RAG chain that:\n",
    "- Uses `TavilySearchAPIRetriever` to get latest AI news\n",
    "- Uses `WikipediaRetriever` to get background on AI topics\n",
    "- Combines both sources to provide a comprehensive news summary\n",
    "\n",
    "#### Exercise 4: Hybrid Retrieval System (üéì Intermediate)\n",
    "Build a system that:\n",
    "- Classifies queries into \"academic\", \"general\", or \"current_events\"\n",
    "- Routes to the appropriate retriever based on query type\n",
    "- Returns results from the most relevant source\n",
    "\n",
    "#### Exercise 5: Multilingual Knowledge Base (üöÄ Advanced)\n",
    "Create a system that:\n",
    "- Detects the language of the user's query\n",
    "- Uses `WikipediaRetriever` with the appropriate language setting\n",
    "- Returns answers in the user's language\n",
    "\n",
    "---\n",
    "\n",
    "### üîó Next Steps\n",
    "\n",
    "- **Notebook 09**: Advanced Retrieval Techniques (Hybrid Search, Re-ranking)\n",
    "- **Notebook 10**: Production RAG Systems (Caching, Monitoring, Scaling)\n",
    "- **LangChain Documentation**: https://python.langchain.com/docs/integrations/retrievers/\n",
    "\n",
    "---\n",
    "\n",
    "### üìö Additional Resources\n",
    "\n",
    "- **ArXiv**: https://arxiv.org/\n",
    "- **Wikipedia API**: https://www.mediawiki.org/wiki/API:Main_page\n",
    "- **Tavily API**: https://tavily.com/\n",
    "- **LangChain Retrievers**: https://python.langchain.com/docs/modules/data_connection/retrievers/\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** üéâ You've mastered external index retrievers in LangChain!\n",
    "\n",
    "You can now build RAG systems that access:\n",
    "- üìÑ Academic research (ArXiv)\n",
    "- üìñ General knowledge (Wikipedia)\n",
    "- üåê Real-time web data (Tavily)\n",
    "\n",
    "Keep experimenting and building amazing AI applications! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simplerag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
